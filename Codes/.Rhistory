show(fit)
## ----session_info-------------------------------------------------------------
sessionInfo()
setwd("/Users/giorgiomontesi/Desktop/Universita_di_Siena/A_PhD_Project/Biomarker_Prediction/ensembleBP/Codes")
options(expressions = 5e5)
library(MLSeq)
library(DESeq2)
library(edgeR)
library(VennDiagram)
library(pamr)
library(caret)
library(devtools)
#install_github("enriquea/feseR", force = T)
library(feseR)
#' @description Import dfCount and dfPheno given their paths
#' @param pathdf relative path to dfCount
#' @param pathclin relative path to dfPheno
#' @returns df: dfCount with samples on cols and genes on rows
#' @returns class: S4 DF dfPheno matched to df with only one col of response variable named condition
dfs.import <- function(pathdf = "../Data/ACC_Adrenocortical_Carcinoma/ACC_Count.csv",
pathclin = "../Data/ACC_Adrenocortical_Carcinoma/ACC_Pheno.csv"){
# Import first df
df <- read.csv2(pathdf, row.names = 1)
df_pheno <- read.csv2(pathclin, row.names = 1)
df <- as.data.frame(t(df))
# Select from df_pheno the only col we are interested in:
df_pheno <- df_pheno[,c(1,9)]
# transform alive status into factor
# L: alive
# D: dead
# match df_count and df_pheno
m <- match(colnames(df), rownames(df_pheno))
df_pheno <- df_pheno[m, ]
df_pheno$patient.vital_status <- as.factor(ifelse(df_pheno$patient.vital_status == "alive", "L", "D"))
df_pheno <- DataFrame(condition = df_pheno$patient.vital_status)
class <- df_pheno
return(list(df, class))
}
#' @description Split dfCount and Class into train and test according split ratio
#' @param df dfCount as preprocessed from dfs.import
#' @param class S4 dfPheno matched and preprocessed as from dfs.import
#' @param ratio split ratio of test set
#' @param mincorr correlation threshold for soft filter
#' @returns data.trainS4
#' @returns data.testS4
#' @returns classts: real test labels
trainTest.split <- function(df, class, ratio = 0.3, mincorr = 0.1, seed = 123){
set.seed(seed)
data <- df
nTest <- ceiling(ncol(data) * ratio)
ind <- sample(ncol(data), nTest, FALSE)
# Minimum count is set to 1 in order to prevent 0 division problem within
# classification models.
data.train <- as.matrix(data[ ,-ind] + 1)
data.test <- as.matrix(data[ ,ind] + 1)
classtr <- DataFrame(condition = class[-ind, ])
classts <- DataFrame(condition = class[ind, ])
# Apply very basic correlation filter to train df
classtr.num <- c(1,0)[classtr$condition]
dtr <- filter.corr(scale(t(data.train), center = T, scale = T),
classtr.num, mincorr = mincorr)
dtr <- (t(dtr))
data.train <- data.train[rownames(data.train) %in% rownames(dtr), ]
# Apply results to test dataset
data.test <- data.test[rownames(data.test) %in% rownames(data.train), ]
# Define S4 objects
data.trainS4 = DESeqDataSetFromMatrix(countData = data.train, colData = classtr,
design = formula(~condition))
data.testS4 = DESeqDataSetFromMatrix(countData = data.test, colData = classts,
design = formula(~condition))
return(list(data.trainS4, data.testS4, classts))
}
#' @description Tests several SVM-based classifiers
#' @param data.trainS4
#' @param data.testS4
#' @param classts
#' @param tL tune Length
#' @param n number of CV
#' @param r number of repeats for CV
#' @returns A list of Confusion Matrices one for each svm-based method
svm.based <- function(data.trainS4, data.testS4, classts,
tL = 2, n = 5, r = 2){
# Define control function for all svm.based classifiers
svmControl <- trainControl(method = "repeatedcv", number = n,
repeats = r, classProbs = TRUE)
print("Fitting SVM-Radial")
set.seed(1510)
# Support vector machines with radial basis function kernel
fit.svmRadial <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = svmControl)
#Predicted class labels
pred.svmRadial <- predict(fit.svmRadial, data.testS4)
pred.svmRadial <- relevel(pred.svmRadial, ref = "D")
actual <- relevel(classts$condition, ref = "D")
tblRadial <- table(Predicted = pred.svmRadial, Actual = actual)
svmRadial.cm <- confusionMatrix(tblRadial, positive = "D")
genes_svmRadial_SVMBased <- list(colnames(fit.svmRadial@modelInfo@trainedModel$trainingData[, c(fit.svmRadial@modelInfo@trainedModel$finalModel@SVindex)]))
print("Fitting SVM-Poly")
set.seed(1510)
# Support vector machines with poly basis function kernel
fit.svmPoly <- classify(data = data.trainS4, method = "svmPoly",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = svmControl)
#Predicted class labels
pred.svmPoly <- predict(fit.svmPoly, data.testS4)
pred.svmPoly <- relevel(pred.svmPoly, ref = "D")
tblPoly <- table(Predicted = pred.svmPoly, Actual = actual)
svmPoly.cm <- confusionMatrix(tblPoly, positive = "D")
genes_svmPoly_SVMBased <- list(colnames(fit.svmPoly@modelInfo@trainedModel$trainingData[, c(fit.svmPoly@modelInfo@trainedModel$finalModel@SVindex)]))
print("Fitting SVM-Linear")
set.seed(1510)
# Support vector machines with linear basis function kernel
fit.svmLinear <- classify(data = data.trainS4, method = "svmLinear",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = svmControl)
#Predicted class labels
pred.svmLinear <- predict(fit.svmLinear, data.testS4)
pred.svmLinear <- relevel(pred.svmLinear, ref = "D")
tblLinear <- table(Predicted = pred.svmLinear, Actual = actual)
svmLinear.cm <- confusionMatrix(tblLinear, positive = "D")
genes_svmLinear_SVMBased <- list(colnames(fit.svmLinear@modelInfo@trainedModel$trainingData[, c(fit.svmLinear@modelInfo@trainedModel$finalModel@SVindex)]))
print("Successfully accomplished SVM-based methods")
return(list(svmRadial.cm, svmPoly.cm, svmLinear.cm,
genes_svmRadial_SVMBased, genes_svmPoly_SVMBased,genes_svmLinear_SVMBased))
}
#' @description Tests several sparse-based classifiers
#' @param data.trainS4
#' @param data.testS4
#' @param classts
#' @param tL tune Length
#' @param n number of CV
#' @param r number of repeats for CV
#' @returns A list of Confusion Matrices one for each sparse-based method
sparse.based <- function(data.trainS4, data.testS4, classts,
tL = 2, n = 2, r = 2){
library(sdwd)
library(sparseLDA)
library(spls)
# Define control function for all sparse.based classifiers
sparseControl <- trainControl(method = "repeatedcv", number = n,
repeats = r, classProbs = TRUE)
actual <- relevel(classts$condition, ref = "D")
print("Fitting sparseLDA")
set.seed(1510)
# Sparse linear discriminant analysis
fit.sparseLDA <- classify(data = data.trainS4, method = "sparseLDA",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = sparseControl)
#Predicted class labels
pred.sparseLDA <- predict(fit.sparseLDA, data.testS4)
pred.sparseLDA <- relevel(pred.sparseLDA, ref = "D")
tblsparseLDA <- table(Predicted = pred.sparseLDA, Actual = actual)
sparseLDA.cm <- confusionMatrix(tblsparseLDA, positive = "D")
genes_sparseLDA_LDAbased <- list(genes_sparseLDA_LDAbased = fit.sparseLDA@modelInfo@trainedModel[["finalModel"]][["varNames"]])
print("Successfully accomplished sparseLDA")
return(list(sparseLDA.cm, genes_sparseLDA_LDAbased))
}
dfsImport <- dfs.import()
df <- dfsImport[[1]]
#df <- df[1:1500, ]
class <- dfsImport[[2]]
keep <- rowSums(df > 10) > round(ncol(df)/3)
df <- df[keep, ]
seed=123
mincorr = 0.4
tts <- trainTest.split(df, class, mincorr = mincorr, seed = seed)
data.trainS4 <- tts[[1]]
data.testS4 <- tts[[2]]
classts <- tts[[3]]
# mini-check per vedere se i geni filtati sono gli stessi
sum(rownames(assay(data.trainS4)) == rownames(assay(data.testS4)))
tL = 20
n = 10
r = 10
# Define control function for all voom.based classifiers
voomControl <- voomControl(method = "repeatedcv", number = n, repeats = r,
tuneLength = tL)
print("Fitting voom-DLDA")
set.seed(1510)
# voomDLDA
fit.voomDLDA <- classify(data = data.trainS4, method = "voomDLDA",
normalize = "deseq", ref = "D",
control = voomControl)
View(fit.voomDLDA)
fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]]
fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]][[1]]
fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]][1]
fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]][,1]
names(fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedSD.pooled"]]
rownames(fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
# Compute elbow genes
coeff_voomDLDA <- fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
names(coeff_voomDLDA) <- rownames(fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
coeff_voomDLDA
# Supponiamo di avere i coefficienti salvati in un vettore chiamato 'coefficients'
# coefficients <- abs(rnorm(100))  # Esempio di coefficienti
# names(coefficients)<-as.character(1:100)
coefficients <- coeff_voomDLDA
# 1. Ordina i coefficienti in ordine decrescente
sorted_coefficients <- sort(coefficients, decreasing = TRUE)
# 2. Fitta una curva ai coefficienti ordinati
# Creazione di un dataframe con i dati ordinati
data <- data.frame(index = seq_along(sorted_coefficients), coefficient = sorted_coefficients)
# Fitta una curva con il pacchetto 'segmented'
fit <- segmented(lm(coefficient ~ index, data = data), seg.Z = ~ index, psi = list(index = 5))
library(dplyr)
library(segmented)
# Fitta una curva con il pacchetto 'segmented'
fit <- segmented(lm(coefficient ~ index, data = data), seg.Z = ~ index, psi = list(index = 5))
# 3. Trova il punto di gomito sulla curva fittata
elbow_point <- round(summary(fit)$psi[[2]])
# Plot dei dati e della curva fittata
#plot(data$index, data$coefficient, type = "l", main = "Elbow Method", xlab = "Index", ylab = "Coefficient")
plot(sorted_coefficients)
lines(data$index, predict(fit), col = "red")
abline(v = elbow_point, col = "blue", lty = 2)
legend("topright", legend = c("Data", "Fitted Curve", "Elbow Point"), col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Stampa il punto di gomito
print(paste("Elbow Point:", elbow_point))
selected_features<-names(sorted_coefficients)[1:elbow_point]
selected_features
elbow_comp <- function(coefficients){
library(dplyr)
library(segmented)
# Supponiamo di avere i coefficienti salvati in un vettore chiamato 'coefficients'
# coefficients <- abs(rnorm(100))  # Esempio di coefficienti
# names(coefficients)<-as.character(1:100)
coefficients <- coefficients
# 1. Ordina i coefficienti in ordine decrescente
sorted_coefficients <- sort(coefficients, decreasing = TRUE)
# 2. Fitta una curva ai coefficienti ordinati
# Creazione di un dataframe con i dati ordinati
data <- data.frame(index = seq_along(sorted_coefficients), coefficient = sorted_coefficients)
# Fitta una curva con il pacchetto 'segmented'
fit <- segmented(lm(coefficient ~ index, data = data), seg.Z = ~ index, psi = list(index = 5))
# 3. Trova il punto di gomito sulla curva fittata
elbow_point <- round(summary(fit)$psi[[2]])
# Plot dei dati e della curva fittata
#plot(data$index, data$coefficient, type = "l", main = "Elbow Method", xlab = "Index", ylab = "Coefficient")
plot(sorted_coefficients)
lines(data$index, predict(fit), col = "red")
abline(v = elbow_point, col = "blue", lty = 2)
legend("topright", legend = c("Data", "Fitted Curve", "Elbow Point"), col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Stampa il punto di gomito
print(paste("Elbow Point:", elbow_point))
selected_features<-names(sorted_coefficients)[1:elbow_point]
return(selected_features)
}
elbow_comp <- function(coefficients){
library(dplyr)
library(segmented)
# Supponiamo di avere i coefficienti salvati in un vettore chiamato 'coefficients'
# coefficients <- abs(rnorm(100))  # Esempio di coefficienti
# names(coefficients)<-as.character(1:100)
coefficients <- coefficients
# 1. Ordina i coefficienti in ordine decrescente
sorted_coefficients <- sort(coefficients, decreasing = TRUE)
# 2. Fitta una curva ai coefficienti ordinati
# Creazione di un dataframe con i dati ordinati
data <- data.frame(index = seq_along(sorted_coefficients), coefficient = sorted_coefficients)
# Fitta una curva con il pacchetto 'segmented'
fit <- segmented(lm(coefficient ~ index, data = data), seg.Z = ~ index, psi = list(index = 5))
# 3. Trova il punto di gomito sulla curva fittata
elbow_point <- round(summary(fit)$psi[[2]])
# Plot dei dati e della curva fittata
#plot(data$index, data$coefficient, type = "l", main = "Elbow Method", xlab = "Index", ylab = "Coefficient")
plot(sorted_coefficients)
lines(data$index, predict(fit), col = "red")
abline(v = elbow_point, col = "blue", lty = 2)
legend("topright", legend = c("Data", "Fitted Curve", "Elbow Point"), col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Stampa il punto di gomito
print(paste("Elbow Point:", elbow_point))
selected_features<-names(sorted_coefficients)[1:elbow_point]
return(selected_features)
}
genes_voomDLDA_voomBased <- elbow_comp(coefficients = coeff_voomDLDA)
genes_voomDLDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDLDA))
View(genes_voomDLDA_voomBased)
set.seed(1510)
# voomDQDA
fit.voomDQDA <- classify(data = data.trainS4, method = "voomDQDA",
normalize = "deseq", ref = "D",
control = voomControl)
View(fit.voomDQDA)
fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
rownames(fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
# Compute elbow genes
coeff_voomDQDA <- fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
names(coeff_voomDQDA) <- rownames(fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
genes_voomDQDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDQDA))
View(genes_voomDLDA_voomBased)
View(genes_voomDQDA_voomBased)
genes_voomDQDA_voomBased[[1]]
genes_voomDLDA_voomBased[[1]]
fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]] == fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
set.seed(1310)
# voomDQDA
fit.voomDQDA <- classify(data = data.trainS4, method = "voomDQDA",
normalize = "deseq", ref = "D",
control = voomControl)
# Compute elbow genes
coeff_voomDQDA <- fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
names(coeff_voomDQDA) <- rownames(fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
genes_voomDQDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDQDA))
View(genes_voomDLDA_voomBased)
View(genes_voomDQDA_voomBased)
set.seed(123)
# voomDQDA
fit.voomDQDA <- classify(data = data.trainS4, method = "voomDQDA",
normalize = "deseq", ref = "D",
control = voomControl)
# Compute elbow genes
coeff_voomDQDA <- fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
names(coeff_voomDQDA) <- rownames(fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
genes_voomDQDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDQDA))
set.seed(1510)
# voomNSC
fit.voomNSC <- classify(data = data.trainS4, method = "voomNSC",
normalize = "deseq", ref = "D",
control = voomControl)
View(fit.voomNSC)
fit.voomNSC@modelInfo@trainedModel@finalModel[["model"]][["weightedMean"]]
fit.voomNSC@modelInfo@trainedModel@finalModel[["model"]][["delta"]]
fit.voomNSC@modelInfo@trainedModel@finalModel[["model"]][["weightedSD.pooled"]]
# Compute elbow genes
coeff_voomNSC <- fit.voomNSC@modelInfo@trainedModel@finalModel[["model"]][["weightedSD.pooled"]]
genes_voomNSC_voomBased <- list(elbow_comp(coefficients = coeff_voomNSC))
coeff_voomDQDA <- fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedSD.pooled"]]
View(fit.voomDQDA)
fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedSD.pooled"]]
coeff_voomDQDA <- fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedSD.pooled"]]
genes_voomDQDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDQDA))
# Compute elbow genes
coeff_voomDLDA <- fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedSD.pooled"]]
genes_voomDLDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDLDA))
fit.voomNSC@modelInfo@trainedModel@finalModel[["model"]][["weightedMean"]] == fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
# Compute elbow genes
coeff_voomNSC <- fit.voomNSC@modelInfo@trainedModel@finalModel[["model"]][["weightedMean"]]
names(coeff_voomNSC) <- rownames(fit.voomNSC@modelInfo@trainedModel@finalModel[["model"]][["weightedMean.C"]])
genes_voomNSC_voomBased <- list(elbow_comp(coefficients = coeff_voomNSC))
# Compute elbow genes
coeff_voomDQDA <- fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
names(coeff_voomDQDA) <- rownames(fit.voomDQDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
genes_voomDQDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDQDA))
# Compute elbow genes
coeff_voomDLDA <- fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean"]]
names(coeff_voomDLDA) <- rownames(fit.voomDLDA@modelInfo@trainedModel@finalModel[["model"]][["weightedStats"]][["weightedMean.C"]])
genes_voomDLDA_voomBased <- list(elbow_comp(coefficients = coeff_voomDLDA))
# Define control function for all voom.based classifiers
linearControl <- discreteControl(method = "repeatedcv", number = n, repeats = r,
tuneLength = tL)
print("Fitting PLDA")
set.seed(1510)
# PLDA
fit.PLDA <- classify(data = data.trainS4, method = "PLDA",
normalize = "deseq", ref = "D",
control = linearControl)
genes_PLDA_LDABased <- list(selectedGenes(fit.PLDA))
View(genes_PLDA_LDABased)
# PLDA2
fit.PLDA2 <- classify(data = data.trainS4, method = "PLDA2",
normalize = "deseq", ref = "D",
control = linearControl)
genes_PLDA2_LDABased <- list(selectedGenes(fit.PLDA2))
View(fit.PLDA2)
View(genes_PLDA2_LDABased)
print("Fitting NBLDA")
set.seed(1510)
# NBLDA
fit.NBLDA <- classify(data = data.trainS4, method = "NBLDA",
normalize = "deseq", ref = "D",
control = linearControl)
View(fit.NBLDA)
fit.NBLDA@modelInfo@trainedModel@finalModel[["ds"]]
# Define control function for all NNet.based classifiers
nnetControl <- trainControl(method = "repeatedcv", number = n,
repeats = r, classProbs = TRUE)
set.seed(1510)
# Sparse linear discriminant analysis
fit.mlp <- classify(data = data.trainS4, method = "mlp",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = nnetControl)
View(fit.mlp)
# Define control function for all NNet.based classifiers
treeControl <- trainControl(method = "repeatedcv", number = n,
repeats = r, classProbs = TRUE,
savePredictions = "all", returnData = T)
# Sparse linear discriminant analysis
fit.cforest <- classify(data = data.trainS4, method = "cforest",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = treeControl)
# Sparse partial least squares
fit.ctree <- classify(data = data.trainS4, method = "ctree",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = treeControl)
# Sparse partial least squares
fit.rf <- classify(data = data.trainS4, method = "rf",
preProcessing = "deseq-vst", ref = "D", tuneLength = tL,
control = treeControl)
View(fit.cforest)
fit.cforest@modelInfo@trainedModel[["finalModel"]]@prediction_weights
View(fit.ctree)
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree
fit.ctree@modelInfo@trainedModel[["finalModel"]]@weights
fit.ctree@modelInfo@trainedModel[["finalModel"]]@where
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[["right"]]
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[["left"]]
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[1]
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[["criterion"]][["criterion"]]
coefficients <- fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[["criterion"]][["criterion"]]
elbow_comp <- function(coefficients){
library(dplyr)
library(segmented)
# Supponiamo di avere i coefficienti salvati in un vettore chiamato 'coefficients'
# coefficients <- abs(rnorm(100))  # Esempio di coefficienti
# names(coefficients)<-as.character(1:100)
coefficients <- coefficients
# 1. Ordina i coefficienti in ordine decrescente
sorted_coefficients <- sort(coefficients, decreasing = TRUE)
# 2. Fitta una curva ai coefficienti ordinati
# Creazione di un dataframe con i dati ordinati
data <- data.frame(index = seq_along(sorted_coefficients), coefficient = sorted_coefficients)
# Fitta una curva con il pacchetto 'segmented'
fit <- segmented(lm(coefficient ~ index, data = data), seg.Z = ~ index, psi = list(index = 5))
# 3. Trova il punto di gomito sulla curva fittata
elbow_point <- round(summary(fit)$psi[[2]])
# Plot dei dati e della curva fittata
#plot(data$index, data$coefficient, type = "l", main = "Elbow Method", xlab = "Index", ylab = "Coefficient")
plot(sorted_coefficients)
lines(data$index, predict(fit), col = "red")
abline(v = elbow_point, col = "blue", lty = 2)
legend("topright", legend = c("Data", "Fitted Curve", "Elbow Point"), col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Stampa il punto di gomito
print(paste("Elbow Point:", elbow_point))
selected_features<-names(sorted_coefficients)[1:elbow_point]
return(selected_features)
}
coefficients
# 1. Ordina i coefficienti in ordine decrescente
sorted_coefficients <- sort(coefficients, decreasing = TRUE)
sorted_coefficients
# 2. Fitta una curva ai coefficienti ordinati
# Creazione di un dataframe con i dati ordinati
data <- data.frame(index = seq_along(sorted_coefficients), coefficient = sorted_coefficients)
# Fitta una curva con il pacchetto 'segmented'
fit <- segmented(lm(coefficient ~ index, data = data), seg.Z = ~ index, psi = list(index = 5))
# 3. Trova il punto di gomito sulla curva fittata
elbow_point <- round(summary(fit)$psi[[2]])
# Plot dei dati e della curva fittata
#plot(data$index, data$coefficient, type = "l", main = "Elbow Method", xlab = "Index", ylab = "Coefficient")
plot(sorted_coefficients)
lines(data$index, predict(fit), col = "red")
abline(v = elbow_point, col = "blue", lty = 2)
legend("topright", legend = c("Data", "Fitted Curve", "Elbow Point"), col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Stampa il punto di gomito
print(paste("Elbow Point:", elbow_point))
selected_features<-names(sorted_coefficients)[1:elbow_point]
selected_features
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree
names(fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree)
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[["psplit"]]
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[["criterion"]][["statistic"]]
coefficients <- fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree[["criterion"]][["statistic"]]
# 1. Ordina i coefficienti in ordine decrescente
sorted_coefficients <- sort(coefficients, decreasing = TRUE)
# 2. Fitta una curva ai coefficienti ordinati
# Creazione di un dataframe con i dati ordinati
data <- data.frame(index = seq_along(sorted_coefficients), coefficient = sorted_coefficients)
# Fitta una curva con il pacchetto 'segmented'
fit <- segmented(lm(coefficient ~ index, data = data), seg.Z = ~ index, psi = list(index = 5))
# 3. Trova il punto di gomito sulla curva fittata
elbow_point <- round(summary(fit)$psi[[2]])
# Plot dei dati e della curva fittata
#plot(data$index, data$coefficient, type = "l", main = "Elbow Method", xlab = "Index", ylab = "Coefficient")
plot(sorted_coefficients)
lines(data$index, predict(fit), col = "red")
abline(v = elbow_point, col = "blue", lty = 2)
legend("topright", legend = c("Data", "Fitted Curve", "Elbow Point"), col = c("black", "red", "blue"), lty = c(1, 1, 2))
# Stampa il punto di gomito
print(paste("Elbow Point:", elbow_point))
selected_features<-names(sorted_coefficients)[1:elbow_point]
selected_features
fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree
a <- list(fit.ctree@modelInfo@trainedModel[["finalModel"]]@tree)
View(a)
a[[1]]
names(a)
a[[1]]
names(a[[1]])
a[[1]][["nodeID"]]
a[[1]][["weights"]]
a[[1]][["criterion"]]
a
a[[1]]
a[[1]][["left"]]
a[[1]][[1]]
a[[1]][[2]]
a[[1]][[3]]
a[[1]]
printcp(a)
library(rpart)
printcp(a)
fit.ctree
fit.ctree@modelInfo@trainedModel[["finalModel"]]
data(fit.ctree@modelInfo@trainedModel[["finalModel"]])
View(fit.ctree@modelInfo@trainedModel[["finalModel"]]@prediction_weights)
