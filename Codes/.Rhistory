## ---- echo=FALSE--------------------------------------------------------------
data(X)
data(y)
y
View(X)
data(beta)
beta
rep(0,p)
actives
beta
## ----WLogit model, eval=FALSE-------------------------------------------------
mod <- WhiteningLogit(X = X, y = y)
## ---- echo=FALSE--------------------------------------------------------------
data(test)
View(test)
mod <- test
## ----beta---------------------------------------------------------------------
beta_min <- mod$beta.min
head(beta_min)
## ----variable selection,fig.width=4,fig.height=3------------------------------
beta_min <- mod$beta.min
df_beta <- data.frame(beta_est=beta_min, Status = ifelse(beta==0, "non-active", "active"))
View(df_beta)
df_plot <- df_beta[which(beta_min!=0), ]
df_plot$index <- which(beta_min!=0)
ggplot2::ggplot(data=df_plot, mapping=aes(y=beta_est, x=index, color=Status))+geom_point()+
theme_bw()+ylab("Estimated coefficients")+xlab("Indices of selected variables")
## ----lasso--------------------------------------------------------------------
library(glmnet)
cvfit = cv.glmnet(X, y, family = "binomial", type.measure = "class", intercept=FALSE)
## ----res lasso----------------------------------------------------------------
beta_lasso <- coef(cvfit, s = "lambda.min")
head(beta_lasso)
## ----lasso selection,fig.width=4,fig.height=3---------------------------------
beta_lasso <- as.vector(beta_lasso)[-1]
df_beta <- data.frame(beta_est=beta_lasso, Status = ifelse(beta==0, "non-active", "active"))
df_plot <- df_beta[which(beta_lasso!=0), ]
df_plot$index <- which(beta_lasso!=0)
ggplot2::ggplot(data=df_plot, mapping=aes(y=beta_est, x=index, color=Status))+geom_point()+
theme_bw()+ylab("Estimated coefficients by glmnet")+xlab("Indices of selected variables")
beta_lasso
## ----WLogit model, eval=FALSE-------------------------------------------------
mod <- WhiteningLogit(X = X, y = y)
## ----beta---------------------------------------------------------------------
beta_min <- mod$beta.min
head(beta_min)
## ----variable selection,fig.width=4,fig.height=3------------------------------
beta_min <- mod$beta.min
df_beta <- data.frame(beta_est=beta_min, Status = ifelse(beta==0, "non-active", "active"))
df_plot <- df_beta[which(beta_min!=0), ]
df_plot$index <- which(beta_min!=0)
ggplot2::ggplot(data=df_plot, mapping=aes(y=beta_est, x=index, color=Status))+geom_point()+
theme_bw()+ylab("Estimated coefficients")+xlab("Indices of selected variables")
ggplot2::ggplot(data=df_plot, mapping=aes(y=beta_est, x=index, color=Status))+geom_point()+
theme_bw()+ylab("Estimated coefficients by glmnet")+xlab("Indices of selected variables")
WhiteningLogit
## ---- echo=FALSE--------------------------------------------------------------
data(test)
mod <- test
## ----beta---------------------------------------------------------------------
beta_min <- mod$beta.min
head(beta_min)
## ----variable selection,fig.width=4,fig.height=3------------------------------
beta_min <- mod$beta.min
df_beta <- data.frame(beta_est=beta_min, Status = ifelse(beta==0, "non-active", "active"))
df_plot <- df_beta[which(beta_min!=0), ]
df_plot$index <- which(beta_min!=0)
ggplot2::ggplot(data=df_plot, mapping=aes(y=beta_est, x=index, color=Status))+geom_point()+
theme_bw()+ylab("Estimated coefficients")+xlab("Indices of selected variables")
c(F,F)||F
F||F
F||T
T||T
T||F
F|F
F|T
T|F
T|T
a <- CalculPx(X, beta_min)
a <- round(CalculPx(X, beta_min))
table(a,y)
## ----WLogit model, eval=FALSE-------------------------------------------------
mod <- WhiteningLogit(X = X, y = y)
## ----beta---------------------------------------------------------------------
beta_min <- mod$beta.min
head(beta_min)
## ----variable selection,fig.width=4,fig.height=3------------------------------
beta_min <- mod$beta.min
a <- round(CalculPx(X, beta_min))
table(a,y)
a <- round(CalculPx(X+1, beta_min))
table(a,y)
citation()
BiocManager::install("MLSeq")
library(MLSeq)
printAvailableMethods()
## ----knitr_options, echo=FALSE, results="hide", warning=FALSE-----------------
library(knitr)
opts_chunk$set(tidy = FALSE, dev = "pdf", fig.show = "hide", message = FALSE, fig.align = "center", cache = FALSE)
## ----load_packages, echo=FALSE, results="hide", warning=FALSE-----------------
library(MLSeq)
library(DESeq2)
library(edgeR)
library(VennDiagram)
library(pamr)
library(caret)
## ----file_path_cervical-------------------------------------------------------
filepath <- system.file("extdata/cervical.txt", package = "MLSeq")
## ----read_cervical_data-------------------------------------------------------
cervical <- read.table(filepath, header=TRUE)
View(cervical)
## ----head_cervical------------------------------------------------------------
head(cervical[ ,1:10]) # Mapped counts for first 6 features of 10 subjects.
## ----define_class_labels------------------------------------------------------
class <- DataFrame(condition = factor(rep(c("N","T"), c(29, 29))))
View(class)
class
## ----data_splitting-----------------------------------------------------------
library(DESeq2)
set.seed(2128)
# We do not perform a differential expression analysis to select differentially
# expressed genes. However, in practice, DE analysis might be performed before
# fitting classifiers. Here, we selected top 100 features having the highest
# gene-wise variances in order to decrease computational cost.
vars <- sort(apply(cervical, 1, var, na.rm = TRUE), decreasing = TRUE)
data <- cervical[names(vars)[1:100], ]
nTest <- ceiling(ncol(data) * 0.3)
ind <- sample(ncol(data), nTest, FALSE)
# Minimum count is set to 1 in order to prevent 0 division problem within
# classification models.
data.train <- as.matrix(data[ ,-ind] + 1)
data.test <- as.matrix(data[ ,ind] + 1)
classtr <- DataFrame(condition = class[-ind, ])
classts <- DataFrame(condition = class[ind, ])
## ----DESeqDataSets------------------------------------------------------------
data.trainS4 = DESeqDataSetFromMatrix(countData = data.train, colData = classtr,
design = formula(~condition))
data.testS4 = DESeqDataSetFromMatrix(countData = data.test, colData = classts,
design = formula(~condition))
## ----Optimizing_model_parameters_example, eval = TRUE, echo = TRUE------------
set.seed(2128)
# Support vector machines with radial basis function kernel
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "T", tuneLength = 10,
control = trainControl(method = "repeatedcv", number = 5,
repeats = 10, classProbs = TRUE))
show(fit.svm)
## ----fitted_model_svm---------------------------------------------------------
trained(fit.svm)
## ----eval = FALSE-------------------------------------------------------------
plot(fit.svm)
## ----fitted_model_svm_figure, echo = FALSE, results='hide'--------------------
cairo_pdf(filename = "fitted_model_svm_figure.pdf", height = 5.5)
plot(fit.svm)
dev.off()
## ----echo = FALSE-------------------------------------------------------------
# Define control list
ctrl.voomDLDA <- voomControl(method = "repeatedcv", number = 5, repeats = 1,
tuneLength = 10)
# Voom-based diagonal linear discriminant analysis
fit.voomDLDA <- classify(data = data.trainS4, method = "voomDLDA",
normalize = "deseq", ref = "T", control = ctrl.voomDLDA)
## -----------------------------------------------------------------------------
trained(fit.voomDLDA)
## -----------------------------------------------------------------------------
#Predicted class labels
pred.svm <- predict(fit.svm, data.testS4)
pred.svm
## -----------------------------------------------------------------------------
pred.svm <- relevel(pred.svm, ref = "T")
actual <- relevel(classts$condition, ref = "T")
tbl <- table(Predicted = pred.svm, Actual = actual)
confusionMatrix(tbl, positive = "T")
## ----results='hide', message=FALSE--------------------------------------------
set.seed(2128)
# Define control lists.
ctrl.continuous <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
ctrl.discrete <- discreteControl(method = "repeatedcv", number = 5, repeats = 10,
tuneLength = 10)
ctrl.voom <- voomControl(method = "repeatedcv", number = 5, repeats = 10,
tuneLength = 10)
# 1. Continuous classifiers, SVM and NSC
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "T", tuneLength = 10,
control = ctrl.continuous)
fit.NSC <- classify(data = data.trainS4, method = "pam",
preProcessing = "deseq-vst", ref = "T", tuneLength = 10,
control = ctrl.continuous)
# 2. Discrete classifiers
fit.plda <- classify(data = data.trainS4, method = "PLDA", normalize = "deseq",
ref = "T", control = ctrl.discrete)
fit.plda2 <- classify(data = data.trainS4, method = "PLDA2", normalize = "deseq",
ref = "T", control = ctrl.discrete)
fit.nblda <- classify(data = data.trainS4, method = "NBLDA", normalize = "deseq",
ref = "T", control = ctrl.discrete)
# 3. voom-based classifiers
fit.voomDLDA <- classify(data = data.trainS4, method = "voomDLDA",
normalize = "deseq", ref = "T", control = ctrl.voom)
fit.voomNSC <- classify(data = data.trainS4, method = "voomNSC",
normalize = "deseq", ref = "T", control = ctrl.voom)
# 4. Predictions
pred.svm <- predict(fit.svm, data.testS4)
pred.NSC <- predict(fit.NSC, data.testS4)
## ----echo = FALSE, results='asis', message=FALSE------------------------------
library(xtable)
pred.svm <- predict(fit.svm, data.testS4)
pred.NSC <- predict(fit.NSC, data.testS4)
pred.plda <- predict(fit.plda, data.testS4)
pred.nblda <- predict(fit.nblda, data.testS4)
pred.voomDLDA <- predict(fit.voomDLDA, data.testS4)
pred.voomNSC <- predict(fit.voomNSC, data.testS4)
actual <- data.testS4$condition
nn <- length(actual)
diag.svm <- sum(diag(table(pred.svm, actual)))
diag.NSC <- sum(diag(table(pred.NSC, actual)))
diag.plda <- sum(diag(table(pred.plda, actual)))
diag.nblda <- sum(diag(table(pred.nblda, actual)))
diag.voomDLDA <- sum(diag(table(pred.voomDLDA, actual)))
diag.voomNSC <- sum(diag(table(pred.voomNSC, actual)))
acc <- c(diag.svm, diag.NSC, diag.plda, diag.nblda, diag.voomDLDA, diag.voomNSC) / nn
sparsity <- c(NA, trained(fit.NSC)$finalModel$nonzero/nrow(data.testS4),
length(selectedGenes(fit.plda))/nrow(data.testS4), NA, NA,
length(selectedGenes(fit.voomNSC))/nrow(data.testS4))
tbl <- data.frame(Classifier = c("SVM", "NSC", "PLDA (Transformed)", "NBLDA", "voomDLDA", "voomNSC"), Accuracy = acc, Sparsity = sparsity)
xtbl <- xtable(tbl, caption = "Classification results for cervical data.", label = "tbl:accRes", align = "lp{4cm}p{2cm}c")
digits(xtbl) <- c(0, 0, 3, 3)
print.xtable(xtbl, caption.placement = "top", include.rownames = FALSE, booktabs = TRUE)
## ----echo = FALSE-------------------------------------------------------------
best_in_accuracy <- as.character(tbl$Classifier[which(acc == max(acc, na.rm = TRUE))])
best_in_acc_text <- paste("\\textbf{", best_in_accuracy, "}", sep = "")
if (length(best_in_accuracy) >= 2){
best_in_acc_text <- paste(paste(best_in_acc_text[-length(best_in_acc_text)], collapse = ", "), best_in_acc_text[length(best_in_acc_text)], sep = " and ")
}
best_in_sparsity <- as.character(tbl$Classifier[which(sparsity == min(sparsity, na.rm = TRUE))])
best_in_sparsity_text <- paste("\\textbf{", best_in_sparsity, "}", sep = "")
if (length(best_in_sparsity) >= 2){
best_in_sparsity_text <- paste(paste(best_in_sparsity_text[-length(best_in_sparsity_text)], collapse = ", "), best_in_sparsity_text[length(best_in_sparsity_text)], sep = " and ")
}
## -----------------------------------------------------------------------------
selectedGenes(fit.voomNSC)
## ----all_common_features, echo = FALSE----------------------------------------
pam.final <- trained(fit.NSC)$finalModel   ## 'pamrtrained' object.
geneIdx <- pamr:::pamr.predict(pam.final, pam.final$xData, threshold = pam.final$threshold, type = "nonzero")
genes.pam <- colnames(pam.final$xData)[geneIdx]
genes.plda <- selectedGenes(fit.plda)
genes.plda2 <- selectedGenes(fit.plda2)
genes.vnsc <- selectedGenes(fit.voomNSC)
tmp.list <- list(genes.pam, genes.plda, genes.plda2, genes.vnsc)
nn <- c(length(genes.pam), length(genes.plda), length(genes.plda2), length(genes.vnsc))
ooo <- order(nn, decreasing = TRUE)
tmp.list <- tmp.list[ooo]
common <- tmp.list[[1]]
for (i in 2:(length(tmp.list))){
tmp2 <- tmp.list[[i]]
tmp <- common[common %in% tmp2]
common <- tmp
}
## ----venn_diagram, echo = FALSE-----------------------------------------------
venn.plot <- venn.diagram(
x = list(voomNSC = genes.vnsc, NSC = genes.pam, PLDA = genes.plda, PLDA2 = genes.plda2),
height = 1200, width = 1200,
resolution = 200,
filename = "Selected_features.png", imagetype = "png",
col = "black",
fill = c("khaki1", "skyblue", "tomato3", "darkolivegreen3"),
alpha = 0.50,
cat.cex = 1.2,
cex = 1.5,
cat.fontface = "bold"
)
## -----------------------------------------------------------------------------
set.seed(2128)
ctrl <- discreteControl(method = "repeatedcv", number = 5, repeats = 2,
tuneLength = 10)
# PLDA without power transformation
fit <- classify(data = data.trainS4, method = "PLDA", normalize = "deseq",
ref = "T", control = ctrl)
show(fit)
## -----------------------------------------------------------------------------
method(fit) <- "PLDA2"
show(fit)
## -----------------------------------------------------------------------------
ref(fit) <- "N"
normalization(fit) <- "TMM"
metaData(fit)
## -----------------------------------------------------------------------------
fit <- update(fit)
show(fit)
## ----echo = FALSE, message=FALSE, error=TRUE----------------------------------
method(fit) <- "rpart"
tmp <- try(update(fit))
## -----------------------------------------------------------------------------
control(fit) <- trainControl(method = "repeatedcv", number = 5, repeats = 2)
# 'normalize' is not valid for continuous classifiers. We use 'preProcessing'
# rather than 'normalize'.
preProcessing(fit) <- "tmm-logcpm"
fit <- update(fit)
show(fit)
## ----session_info-------------------------------------------------------------
sessionInfo()
setwd("/Users/giorgiomontesi/Desktop/Universita_di_Siena/A_PhD_Project/Biomarker_Prediction/ensembleBP/Codes")
library(MLSeq)
library(DESeq2)
library(edgeR)
library(VennDiagram)
library(pamr)
library(caret)
# Import first df
df <- read.csv2("../Data/ACC_Adrenocortical_Carcinoma/ACC_Count.csv", row.names = 1)
df_pheno <- read.csv2("../Data/ACC_Adrenocortical_Carcinoma/ACC_Pheno.csv", row.names = 1)
df <- as.data.frame(t(df))
# Select from df_pheno the only col we are interested in:
df_pheno <- df_pheno[,c(1,9)]
# transform alive status into factor
# 0: alive
# 1: dead
# match df_count and df_pheno
m <- match(colnames(df), rownames(df_pheno))
df_pheno <- df_pheno[m, ]
df_pheno$patient.vital_status <- as.factor(ifelse(df_pheno$patient.vital_status == "alive", "L", "D"))
df_pheno <- DataFrame(condition = df_pheno$patient.vital_status)
## ----define_class_labels------------------------------------------------------
class <- df_pheno
class
## ----data_splitting-----------------------------------------------------------
library(DESeq2)
set.seed(2128)
# We do not perform a differential expression analysis to select differentially
# expressed genes. However, in practice, DE analysis might be performed before
# fitting classifiers. Here, we selected top 100 features having the highest
# gene-wise variances in order to decrease computational cost.
data <- df
nTest <- ceiling(ncol(data) * 0.3)
ind <- sample(ncol(data), nTest, FALSE)
# Minimum count is set to 1 in order to prevent 0 division problem within
# classification models.
data.train <- as.matrix(data[ ,-ind] + 1)
data.test <- as.matrix(data[ ,ind] + 1)
classtr <- DataFrame(condition = class[-ind, ])
classts <- DataFrame(condition = class[ind, ])
## ----DESeqDataSets------------------------------------------------------------
data.trainS4 = DESeqDataSetFromMatrix(countData = data.train, colData = classtr,
design = formula(~condition))
data.testS4 = DESeqDataSetFromMatrix(countData = data.test, colData = classts,
design = formula(~condition))
set.seed(2128)
# Support vector machines with radial basis function kernel
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "D", tuneLength = 2,
control = trainControl(method = "repeatedcv", number = 2,
repeats = 2, classProbs = TRUE))
show(fit.svm)
trained(fit.svm)
View(fit.svm)
plot(fit.svm)
set.seed(2128)
# Support vector machines with radial basis function kernel
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "D", tuneLength = 10,
control = trainControl(method = "repeatedcv", number = 5,
repeats = 10, classProbs = TRUE))
show(fit.svm)
trained(fit.svm)
plot(fit.svm)
## -----------------------------------------------------------------------------
#Predicted class labels
pred.svm <- predict(fit.svm, data.testS4)
pred.svm
## -----------------------------------------------------------------------------
pred.svm <- relevel(pred.svm, ref = "D")
actual <- relevel(classts$condition, ref = "D")
tbl <- table(Predicted = pred.svm, Actual = actual)
confusionMatrix(tbl, positive = "D")
a <- confusionMatrix(tbl, positive = "D")
svmRadial.cm <- confusionMatrix(tbl, positive = "D")
View(classts)
setwd("/Users/giorgiomontesi/Desktop/Universita_di_Siena/A_PhD_Project/Biomarker_Prediction/ensembleBP/Codes")
library(MLSeq)
library(DESeq2)
library(edgeR)
library(VennDiagram)
library(pamr)
library(caret)
#' @description
#' Import dfCount and dfPheno given their paths
#' @param pathdf relative path to dfCount
#' @param pathclin relative path to dfPheno
#' @returns df: dfCount with samples on cols and genes on rows
#' @returns class: S4 DF dfPheno matched to df with only one col of response variable named condition
dfs.import <- function(pathdf = "../Data/ACC_Adrenocortical_Carcinoma/ACC_Count.csv",
pathclin = "../Data/ACC_Adrenocortical_Carcinoma/ACC_Pheno.csv"){
# Import first df
df <- read.csv2(pathdf, row.names = 1)
df_pheno <- read.csv2(pathclin, row.names = 1)
df <- as.data.frame(t(df))
# Select from df_pheno the only col we are interested in:
df_pheno <- df_pheno[,c(1,9)]
# transform alive status into factor
# L: alive
# D: dead
# match df_count and df_pheno
m <- match(colnames(df), rownames(df_pheno))
df_pheno <- df_pheno[m, ]
df_pheno$patient.vital_status <- as.factor(ifelse(df_pheno$patient.vital_status == "alive", "L", "D"))
df_pheno <- DataFrame(condition = df_pheno$patient.vital_status)
class <- df_pheno
return(list(df, class))
}
#' @description
#' Split dfCount and Class into train and test according split ratio
#' @param df dfCount as preprocessed from dfs.import
#' @param class S4 dfPheno matched and preprocessed as from dfs.import
#' @param ratio split ratio of test set
#' @returns data.trainS4
#' @returns data.testS4
trainTest.split <- function(df, class, ratio = 0.3){
set.seed(2128)
data <- df
nTest <- ceiling(ncol(data) * ratio)
ind <- sample(ncol(data), nTest, FALSE)
# Minimum count is set to 1 in order to prevent 0 division problem within
# classification models.
data.train <- as.matrix(data[ ,-ind] + 1)
data.test <- as.matrix(data[ ,ind] + 1)
classtr <- DataFrame(condition = class[-ind, ])
classts <- DataFrame(condition = class[ind, ])
data.trainS4 = DESeqDataSetFromMatrix(countData = data.train, colData = classtr,
design = formula(~condition))
data.testS4 = DESeqDataSetFromMatrix(countData = data.test, colData = classts,
design = formula(~condition))
return(list(data.trainS4, data.testS4))
}
svm.based <- function(data.trainS4, data.testS4){
set.seed(2128)
# Support vector machines with radial basis function kernel
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "D", tuneLength = 2,
control = trainControl(method = "repeatedcv", number = 5,
repeats = 2, classProbs = TRUE))
# show(fit.svm)
# trained(fit.svm)
# plot(fit.svm)
#Predicted class labels
pred.svm <- predict(fit.svm, data.testS4)
pred.svm <- relevel(pred.svm, ref = "D")
actual <- relevel(classts$condition, ref = "D")
tbl <- table(Predicted = pred.svm, Actual = actual)
svmRadial.cm <- confusionMatrix(tbl, positive = "D")
return(list(svmRadial.cm))
}
dfsImport <- dfs.import
df <- dfsImport[[1]]
View(dfsImport)
dfsImport <- dfs.import()
df <- dfsImport[[1]]
class <- dfsImport[[2]]
tts <- trainTest.split(df, class)
data.trainS4 <- tts[[1]]
data.testS4 <- tts[[2]]
svm <- svm.based(data.trainS4, data.testS4)
#' @description
#' Split dfCount and Class into train and test according split ratio
#' @param df dfCount as preprocessed from dfs.import
#' @param class S4 dfPheno matched and preprocessed as from dfs.import
#' @param ratio split ratio of test set
#' @returns data.trainS4
#' @returns data.testS4
trainTest.split <- function(df, class, ratio = 0.3){
set.seed(2128)
data <- df
nTest <- ceiling(ncol(data) * ratio)
ind <- sample(ncol(data), nTest, FALSE)
# Minimum count is set to 1 in order to prevent 0 division problem within
# classification models.
data.train <- as.matrix(data[ ,-ind] + 1)
data.test <- as.matrix(data[ ,ind] + 1)
classtr <- DataFrame(condition = class[-ind, ])
classts <- DataFrame(condition = class[ind, ])
data.trainS4 = DESeqDataSetFromMatrix(countData = data.train, colData = classtr,
design = formula(~condition))
data.testS4 = DESeqDataSetFromMatrix(countData = data.test, colData = classts,
design = formula(~condition))
return(list(data.trainS4, data.testS4, classts))
}
svm.based <- function(data.trainS4, data.testS4, classts){
set.seed(2128)
# Support vector machines with radial basis function kernel
fit.svm <- classify(data = data.trainS4, method = "svmRadial",
preProcessing = "deseq-vst", ref = "D", tuneLength = 2,
control = trainControl(method = "repeatedcv", number = 5,
repeats = 2, classProbs = TRUE))
# show(fit.svm)
# trained(fit.svm)
# plot(fit.svm)
#Predicted class labels
pred.svm <- predict(fit.svm, data.testS4)
pred.svm <- relevel(pred.svm, ref = "D")
actual <- relevel(classts$condition, ref = "D")
tbl <- table(Predicted = pred.svm, Actual = actual)
svmRadial.cm <- confusionMatrix(tbl, positive = "D")
return(list(svmRadial.cm))
}
dfsImport <- dfs.import()
df <- dfsImport[[1]]
class <- dfsImport[[2]]
tts <- trainTest.split(df, class)
data.trainS4 <- tts[[1]]
data.testS4 <- tts[[2]]
classts <- tts[[3]]
svm <- svm.based(data.trainS4, data.testS4, classts)
View(svm)
svm[[1]]
